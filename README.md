# UrbanSound8k_MFCC

## Project Abstract

This study compares three neural network
 models—Artificial Neural Networks (ANN),
 Convolutional Neural Networks (CNN), and
 Recurrent Neural Networks (RNN)—for audio
 classification using the UrbanSound8k dataset.
 We trained each model with identical conditions
 (30 epochs, 75%-25% train-test split) and used
 MEL-Frequency Cepstral Coefficients (MFCCs) as
 features. The ANN model achieved the highest
 accuracy of 93.2%, making it suitable for small,
 well-engineered datasets. The CNN model
 performed well with an accuracy of 88.9%,
 capturing both local and global audio features.
 The RNN model, with LSTM layers, reached 81.5%,
 showing promise for sequential pattern
 recognition.
