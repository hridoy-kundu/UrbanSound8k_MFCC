{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e17290-4fe7-4ca7-9cbc-ca91a95f33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c10f6f7-22e8-49b3-9381-bc55eab760aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID         Class\n",
      "0   0         siren\n",
      "1   1  street_music\n",
      "2   2      drilling\n",
      "3   3         siren\n",
      "4   4      dog_bark\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset containing file IDs and their respective labels\n",
    "df = pd.read_csv('Urban Sound Dataset.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad47a97-c0e0-4677-964d-cf1ff9abf0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract MFCC features from audio files\n",
    "def extract_features(file_path, max_pad_len=174):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        padded_mfccs = np.pad(mfccs, ((0, 0), (0, max(0, max_pad_len - mfccs.shape[1]))), mode='constant')\n",
    "        return padded_mfccs[:, :max_pad_len].T  # Transpose for RNN input (time_steps, features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7401b55-7b5f-4f55-b2ff-68847cd54712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1103\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1323\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python312\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=2048 is too large for input signal of length=1523\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Extract features and labels from the dataset\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    file_path = os.path.join('Train', f\"{row['ID']}.wav\")\n",
    "    mfccs = extract_features(file_path)\n",
    "    if mfccs is not None:\n",
    "        features.append(mfccs)\n",
    "        labels.append(row['Class'])\n",
    "\n",
    "# Encode the labels to integers and convert to one-hot encoding\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "labels_encoded = to_categorical(labels_encoded)\n",
    "\n",
    "# Convert features and labels to NumPy arrays\n",
    "features = np.array(features)\n",
    "labels_encoded = np.array(labels_encoded)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82696eab-cb04-4191-af58-5b57c422c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define the RNN-LSTM model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# First LSTM layer\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Second LSTM layer\n",
    "model.add(LSTM(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Output layer for classification\n",
    "model.add(Dense(labels_encoded.shape[1], activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b72a4fa-c5a4-4e64-9b06-4674072bfa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with optimizer, loss function, and evaluation metric\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97d8923d-82f7-4d36-bcd5-b102ca2f894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 196ms/step - accuracy: 0.1711 - loss: 2.3040 - val_accuracy: 0.4103 - val_loss: 1.7866\n",
      "Epoch 2/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 190ms/step - accuracy: 0.3856 - loss: 1.6382 - val_accuracy: 0.5402 - val_loss: 1.3604\n",
      "Epoch 3/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 184ms/step - accuracy: 0.4971 - loss: 1.3942 - val_accuracy: 0.5805 - val_loss: 1.1955\n",
      "Epoch 4/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 183ms/step - accuracy: 0.5693 - loss: 1.2416 - val_accuracy: 0.6276 - val_loss: 1.0442\n",
      "Epoch 5/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 184ms/step - accuracy: 0.6211 - loss: 1.0752 - val_accuracy: 0.6425 - val_loss: 1.0210\n",
      "Epoch 6/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 178ms/step - accuracy: 0.6619 - loss: 0.9834 - val_accuracy: 0.6517 - val_loss: 1.0580\n",
      "Epoch 7/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 183ms/step - accuracy: 0.6940 - loss: 0.9055 - val_accuracy: 0.6724 - val_loss: 0.9784\n",
      "Epoch 8/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 180ms/step - accuracy: 0.6824 - loss: 0.9259 - val_accuracy: 0.6690 - val_loss: 0.9155\n",
      "Epoch 9/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 182ms/step - accuracy: 0.6976 - loss: 0.8990 - val_accuracy: 0.7092 - val_loss: 0.8468\n",
      "Epoch 10/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 188ms/step - accuracy: 0.7268 - loss: 0.8345 - val_accuracy: 0.7402 - val_loss: 0.7528\n",
      "Epoch 11/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 189ms/step - accuracy: 0.7322 - loss: 0.7862 - val_accuracy: 0.7506 - val_loss: 0.7206\n",
      "Epoch 12/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 186ms/step - accuracy: 0.7480 - loss: 0.7556 - val_accuracy: 0.7414 - val_loss: 0.7580\n",
      "Epoch 13/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 188ms/step - accuracy: 0.7553 - loss: 0.7260 - val_accuracy: 0.7103 - val_loss: 0.8567\n",
      "Epoch 14/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 352ms/step - accuracy: 0.7427 - loss: 0.7481 - val_accuracy: 0.7460 - val_loss: 0.7832\n",
      "Epoch 15/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 356ms/step - accuracy: 0.7783 - loss: 0.6535 - val_accuracy: 0.7161 - val_loss: 0.8295\n",
      "Epoch 16/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 3s/step - accuracy: 0.7726 - loss: 0.6576 - val_accuracy: 0.7575 - val_loss: 0.7131\n",
      "Epoch 17/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 353ms/step - accuracy: 0.8006 - loss: 0.5918 - val_accuracy: 0.7920 - val_loss: 0.6085\n",
      "Epoch 18/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 353ms/step - accuracy: 0.8012 - loss: 0.5950 - val_accuracy: 0.7690 - val_loss: 0.7306\n",
      "Epoch 19/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 347ms/step - accuracy: 0.7568 - loss: 0.7017 - val_accuracy: 0.7379 - val_loss: 0.7630\n",
      "Epoch 20/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 346ms/step - accuracy: 0.7881 - loss: 0.6533 - val_accuracy: 0.7414 - val_loss: 0.7143\n",
      "Epoch 21/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 360ms/step - accuracy: 0.8022 - loss: 0.5881 - val_accuracy: 0.7851 - val_loss: 0.6757\n",
      "Epoch 22/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 356ms/step - accuracy: 0.7920 - loss: 0.5971 - val_accuracy: 0.7299 - val_loss: 0.7913\n",
      "Epoch 23/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 359ms/step - accuracy: 0.7855 - loss: 0.6319 - val_accuracy: 0.7759 - val_loss: 0.6931\n",
      "Epoch 24/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 359ms/step - accuracy: 0.8065 - loss: 0.6015 - val_accuracy: 0.8138 - val_loss: 0.6651\n",
      "Epoch 25/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 342ms/step - accuracy: 0.8251 - loss: 0.5245 - val_accuracy: 0.7839 - val_loss: 0.6688\n",
      "Epoch 26/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 359ms/step - accuracy: 0.8290 - loss: 0.5107 - val_accuracy: 0.8000 - val_loss: 0.6947\n",
      "Epoch 27/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 354ms/step - accuracy: 0.8426 - loss: 0.4988 - val_accuracy: 0.8046 - val_loss: 0.6633\n",
      "Epoch 28/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 356ms/step - accuracy: 0.8536 - loss: 0.4587 - val_accuracy: 0.8218 - val_loss: 0.5800\n",
      "Epoch 29/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 356ms/step - accuracy: 0.8534 - loss: 0.4502 - val_accuracy: 0.8241 - val_loss: 0.5778\n",
      "Epoch 30/30\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 351ms/step - accuracy: 0.8770 - loss: 0.3854 - val_accuracy: 0.7609 - val_loss: 0.8050\n"
     ]
    }
   ],
   "source": [
    "# Train the model using training data and validate on a portion of it\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed8ad8a0-45f3-4748-82f7-2a19c158d135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 145ms/step - accuracy: 0.7731 - loss: 0.7860\n",
      "Test Loss: 0.7583, Test Accuracy: 0.7856\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's performance on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a798d-4a68-4183-8bcf-aa70d9629268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
